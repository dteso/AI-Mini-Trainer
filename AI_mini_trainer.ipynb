{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMM9mmCiVbBPyXDr9dlEXnK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dteso/AI-Mini-Trainer/blob/main/AI_mini_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio scikit-learn pandas plotly atomicwrites"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyc-HbLlkucd",
        "outputId": "07453702-1eea-46f2-b25b-6b54c8b6e58f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting atomicwrites\n",
            "  Downloading atomicwrites-1.4.1.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: atomicwrites\n",
            "  Building wheel for atomicwrites (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atomicwrites: filename=atomicwrites-1.4.1-py2.py3-none-any.whl size=6943 sha256=dbfe1b2bca1033f5794479aa1d27fa3218e77c991b0a3520ec9da7b44ab432e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/99/9c/d24e98c35f30eba0c367ad1e7888d396d676abb35fe1e7611c\n",
            "Successfully built atomicwrites\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, atomicwrites, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 atomicwrites-1.4.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "qquUaEtQkbgu",
        "outputId": "42dd0d61-872d-4f22-a583-40dc2b963d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f9df17f18015dfe9f7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f9df17f18015dfe9f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registro cargado: {'caner.pkl': {'model_path': 'models/caner.pkl', 'headers': ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'], 'target_names': ['malignant', 'benign']}}\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os, json, pickle, warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "import tempfile\n",
        "\n",
        "# ---------------------------\n",
        "# IMPORTACIONES DE SKLEARN\n",
        "# ---------------------------\n",
        "from sklearn.datasets import (\n",
        "    load_iris, load_wine, load_breast_cancer, load_digits,\n",
        "    load_diabetes, fetch_california_housing, load_linnerud,\n",
        "    make_regression, make_friedman1\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, r2_score, mean_squared_error\n",
        "from sklearn.linear_model import (\n",
        "    LogisticRegression, LinearRegression, RidgeClassifier, Ridge, Lasso,\n",
        "    ElasticNet, SGDClassifier, SGDRegressor\n",
        ")\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier,\n",
        "    GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor,\n",
        "    BaggingClassifier, BaggingRegressor, ExtraTreesClassifier, ExtraTreesRegressor,\n",
        "    HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from atomicwrites import atomic_write\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# CONFIGURACIÓN: DATASETS Y MODELOS\n",
        "# ---------------------------\n",
        "available_datasets = {\n",
        "    \"Iris (Clasificación)\": load_iris,\n",
        "    \"Wine (Clasificación)\": load_wine,\n",
        "    \"Breast Cancer (Clasificación)\": load_breast_cancer,\n",
        "    \"Digits (Clasificación)\": load_digits,\n",
        "    \"Linnerud (Clasificación - multietiqueta)\": load_linnerud,\n",
        "    \"Diabetes (Regresión)\": load_diabetes,\n",
        "    \"California Housing (Regresión)\": fetch_california_housing,\n",
        "    \"Friedman1 (Regresión sintética)\": lambda: {\n",
        "        \"data\": make_friedman1(n_samples=200, n_features=10, random_state=42)[0],\n",
        "        \"target\": make_friedman1(n_samples=200, n_features=10, random_state=42)[1],\n",
        "        \"feature_names\": [f\"X{i}\" for i in range(10)]\n",
        "    },\n",
        "    \"Make Regression (Regresión sintética)\": lambda: {\n",
        "        \"data\": make_regression(n_samples=200, n_features=8, noise=0.1, random_state=42)[0],\n",
        "        \"target\": make_regression(n_samples=200, n_features=8, noise=0.1, random_state=42)[1],\n",
        "        \"feature_names\": [f\"X{i}\" for i in range(8)]\n",
        "    },\n",
        "}\n",
        "\n",
        "classification_models = {\n",
        "    \"Logistic Regression\": LogisticRegression,\n",
        "    \"KNN Classifier\": KNeighborsClassifier,\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier,\n",
        "    \"Random Forest Classifier\": RandomForestClassifier,\n",
        "    \"SVC\": SVC,\n",
        "    \"Naive Bayes\": GaussianNB,\n",
        "}\n",
        "\n",
        "regression_models = {\n",
        "    \"Linear Regression\": LinearRegression,\n",
        "    \"Random Forest Regressor\": RandomForestRegressor\n",
        "}\n",
        "\n",
        "available_models = {**classification_models, **regression_models}\n",
        "\n",
        "# ---------------------------\n",
        "# FUNCIONES DE APOYO\n",
        "# ---------------------------\n",
        "def load_dataset(name):\n",
        "    \"\"\"Carga el dataset y retorna (df, target).\"\"\"\n",
        "    dataset = available_datasets[name]()\n",
        "    df = pd.DataFrame(\n",
        "        dataset[\"data\"],\n",
        "        columns=dataset.get(\"feature_names\", [f\"X{i}\" for i in range(dataset[\"data\"].shape[1])])\n",
        "    )\n",
        "    target = pd.Series(dataset[\"target\"], name=\"target\")\n",
        "    return df, target\n",
        "\n",
        "def is_classification_task(target):\n",
        "    \"\"\"Determina si el objetivo (target) es para clasificación.\"\"\"\n",
        "    return pd.Series(target).nunique() < 20 and pd.api.types.is_integer_dtype(target)\n",
        "\n",
        "def show_dataset_with_target(dataset_name):\n",
        "    \"\"\"Para vista de ejemplo: un dataframe con columna 'class' textual, si aplica.\"\"\"\n",
        "    dataset = available_datasets[dataset_name]()\n",
        "    df = pd.DataFrame(\n",
        "        dataset[\"data\"],\n",
        "        columns=dataset.get(\"feature_names\", [f\"X{i}\" for i in range(dataset[\"data\"].shape[1])])\n",
        "    )\n",
        "    if \"target_names\" in dataset:\n",
        "        try:\n",
        "            labels = pd.Series(dataset[\"target\"]).apply(lambda x: dataset[\"target_names\"][x])\n",
        "        except:\n",
        "            labels = dataset[\"target\"]\n",
        "    else:\n",
        "        labels = dataset[\"target\"]\n",
        "    df[\"class\"] = labels\n",
        "    label = f\"Vista del Dataset ({len(df)} elementos)\"\n",
        "    return df, label\n",
        "\n",
        "def update_model_choices(dataset_name):\n",
        "    \"\"\"Según el dataset, selecciona modelos de clasificación o regresión.\"\"\"\n",
        "    df, target = load_dataset(dataset_name)\n",
        "    if is_classification_task(target):\n",
        "        return gr.update(choices=list(classification_models.keys()),\n",
        "                         value=list(classification_models.keys())[0])\n",
        "    else:\n",
        "        return gr.update(choices=list(regression_models.keys()),\n",
        "                         value=list(regression_models.keys())[0])\n",
        "\n",
        "def train_multiple_models(dataset_name, model_names, test_size):\n",
        "    \"\"\"Entrena varios modelos y retorna resultados (incluyendo métrica).\"\"\"\n",
        "    df, target = load_dataset(dataset_name)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=test_size, random_state=42)\n",
        "    results = []\n",
        "    is_classif = is_classification_task(target)\n",
        "    for name in model_names:\n",
        "        ModelClass = available_models[name]\n",
        "        model = ModelClass()\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        if is_classif:\n",
        "            metric = accuracy_score(y_test, y_pred)\n",
        "            report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        else:\n",
        "            metric = r2_score(y_test, y_pred)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            report = {\"R2 Score\": metric, \"MSE\": mse}\n",
        "        results.append({\n",
        "            \"Modelo\": name,\n",
        "            \"Precisión\": metric,\n",
        "            \"Reporte\": report,\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def plot_accuracy_comparison(results):\n",
        "    \"\"\"Barplot comparando la métrica (accuracy o R2).\"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    metrics = [r[\"Precisión\"] for r in results]\n",
        "    sns.barplot(x=[r[\"Modelo\"] for r in results], y=metrics, ax=ax)\n",
        "    if all(0 <= m <= 1 for m in metrics):\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_ylabel(\"Precisión\")\n",
        "    else:\n",
        "        ax.set_ylabel(\"R2 Score\")\n",
        "    ax.set_title(\"Comparación de Modelos\")\n",
        "    ax.tick_params(axis=\"x\", rotation=45)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def export_reports_as_csv(results):\n",
        "    \"\"\"Devuelve un CSV con Modelo / Precisión.\"\"\"\n",
        "    df = pd.DataFrame([{\"Modelo\": r[\"Modelo\"], \"Precisión\": r[\"Precisión\"]} for r in results])\n",
        "    return df.to_csv(index=False)\n",
        "\n",
        "def full_training(dataset_name, selected_models, test_size):\n",
        "    \"\"\"Entrena varios modelos y retorna dataframe, gráfica y CSV.\"\"\"\n",
        "    results = train_multiple_models(dataset_name, selected_models, test_size)\n",
        "    df_resultados = pd.DataFrame(\n",
        "        [{\"Modelo\": r[\"Modelo\"], \"Precisión\": round(r[\"Precisión\"], 4)} for r in results]\n",
        "    ).sort_values(by=\"Precisión\", ascending=False)\n",
        "    fig = plot_accuracy_comparison(results)\n",
        "    csv_str = export_reports_as_csv(results)\n",
        "    csv_path = \"report.csv\"\n",
        "    with open(csv_path, \"w\") as f:\n",
        "        f.write(csv_str)\n",
        "    return df_resultados, fig, csv_path\n",
        "\n",
        "def run_eda(dataset_name):\n",
        "    \"\"\"Devuelve la descripción estadística del dataset.\"\"\"\n",
        "    df, _ = load_dataset(dataset_name)\n",
        "    return df.describe().reset_index()\n",
        "\n",
        "def plot_eda(dataset_name):\n",
        "    \"\"\"Boxplot por feature.\"\"\"\n",
        "    df, _ = load_dataset(dataset_name)\n",
        "    df_melted = df.melt(var_name=\"feature\", value_name=\"valor\")\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.boxplot(data=df_melted, x=\"feature\", y=\"valor\", hue=\"feature\", palette=\"Set2\", dodge=False, ax=ax)\n",
        "    leg = ax.get_legend()\n",
        "    if leg is not None:\n",
        "        leg.remove()\n",
        "    ax.set_title(\"Distribución por Feature (Boxplot)\", fontsize=14, weight=\"bold\")\n",
        "    ax.set_xlabel(\"Feature\", fontsize=12)\n",
        "    ax.set_ylabel(\"Valor\", fontsize=12)\n",
        "    ax.tick_params(axis=\"x\", labelrotation=30)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def run_pca(dataset_name):\n",
        "    \"\"\"PCA a 2 componentes y scatterplot.\"\"\"\n",
        "    df, _ = load_dataset(dataset_name)\n",
        "    pca = PCA(n_components=2)\n",
        "    components = pca.fit_transform(df)\n",
        "    df_pca = pd.DataFrame(components, columns=[\"PC1\", \"PC2\"])\n",
        "    dataset = available_datasets[dataset_name]()\n",
        "    if \"target_names\" in dataset:\n",
        "        try:\n",
        "            target_labels = pd.Series(dataset[\"target\"]).apply(lambda x: dataset[\"target_names\"][x])\n",
        "        except:\n",
        "            target_labels = dataset[\"target\"]\n",
        "    else:\n",
        "        target_labels = dataset[\"target\"]\n",
        "    df_pca[\"Clase\"] = target_labels\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=\"Clase\", palette=\"Set2\", s=60, ax=ax)\n",
        "    ax.set_title(\"PCA - 2 Componentes\", fontsize=14, weight=\"bold\")\n",
        "    ax.set_xlabel(\"PC1\", fontsize=12)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=12)\n",
        "    ax.legend(title=\"Clase\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ---------------------------\n",
        "# HIPERPARÁMETROS DE MODELOS\n",
        "# ---------------------------\n",
        "model_params_demo = {\n",
        "    \"Logistic Regression\": {\n",
        "        \"C\": {\"type\": \"slider\", \"min\": 0.01, \"max\": 10.0, \"value\": 1.0, \"step\": 0.01, \"label\": \"C (Regularización)\"},\n",
        "        \"max_iter\": {\"type\": \"slider\", \"min\": 100, \"max\": 2000, \"value\": 1000, \"step\": 100, \"label\": \"Iteraciones Máx.\"}\n",
        "    },\n",
        "    \"KNN Classifier\": {\n",
        "        \"n_neighbors\": {\"type\": \"slider\", \"min\": 1, \"max\": 30, \"value\": 5, \"step\": 1, \"label\": \"n_neighbors\"}\n",
        "    },\n",
        "    \"Decision Tree Classifier\": {\n",
        "        \"max_depth\": {\"type\": \"slider\", \"min\": 1, \"max\": 20, \"value\": 10, \"step\": 1, \"label\": \"max_depth\"},\n",
        "        \"min_samples_split\": {\"type\": \"slider\", \"min\": 2, \"max\": 20, \"value\": 2, \"step\": 1, \"label\": \"min_samples_split\"}\n",
        "    },\n",
        "    \"Random Forest Classifier\": {\n",
        "        \"n_estimators\": {\"type\": \"slider\", \"min\": 10, \"max\": 200, \"value\": 100, \"step\": 10, \"label\": \"n_estimators\"},\n",
        "        \"max_depth\": {\"type\": \"slider\", \"min\": 1, \"max\": 20, \"value\": 10, \"step\": 1, \"label\": \"max_depth\"}\n",
        "    },\n",
        "    \"SVC\": {\n",
        "        \"C\": {\"type\": \"slider\", \"min\": 0.1, \"max\": 10.0, \"value\": 1.0, \"step\": 0.1, \"label\": \"C\"},\n",
        "        \"gamma\": {\"type\": \"slider\", \"min\": 0.001, \"max\": 1.0, \"value\": 0.01, \"step\": 0.001, \"label\": \"gamma\"}\n",
        "    },\n",
        "    \"Random Forest Regressor\": {\n",
        "        \"n_estimators\": {\"type\": \"slider\", \"min\": 10, \"max\": 200, \"value\": 100, \"step\": 10, \"label\": \"n_estimators\"},\n",
        "        \"max_depth\": {\"type\": \"slider\", \"min\": 1, \"max\": 20, \"value\": 5, \"step\": 1, \"label\": \"max_depth\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "def fix_slider_params(params):\n",
        "    \"\"\"Convierte 'min' y 'max' en 'minimum' y 'maximum' para Gradio.\"\"\"\n",
        "    params = params.copy()\n",
        "    if \"min\" in params:\n",
        "        params[\"minimum\"] = params.pop(\"min\")\n",
        "    if \"max\" in params:\n",
        "        params[\"maximum\"] = params.pop(\"max\")\n",
        "    return params\n",
        "\n",
        "# ---------------------------\n",
        "# GUARDAR/REGISTRAR MODELOS\n",
        "# ---------------------------\n",
        "def save_model_locally(model, model_save_name, headers, target_names=None):\n",
        "    model_dir = \"models\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    if not model_save_name.endswith(\".pkl\"):\n",
        "        model_save_name += \".pkl\"\n",
        "    model_path = os.path.join(model_dir, model_save_name)\n",
        "\n",
        "    # Guarda el modelo junto con la información de cabeceras y target_names (usando pickle)\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump({\"model\": model, \"headers\": headers, \"target_names\": target_names}, f)\n",
        "\n",
        "    # Convertir target_names a lista si es necesario para JSON\n",
        "    if target_names is not None:\n",
        "        if isinstance(target_names, np.ndarray):\n",
        "            target_names_serializable = target_names.tolist()\n",
        "        elif not isinstance(target_names, (list, tuple)):\n",
        "            target_names_serializable = list(target_names)\n",
        "        else:\n",
        "            target_names_serializable = target_names\n",
        "    else:\n",
        "        target_names_serializable = None\n",
        "\n",
        "    # Cargar (o inicializar) el registro actual\n",
        "    registry_path = \"model_registry.json\"\n",
        "    if os.path.exists(registry_path):\n",
        "        try:\n",
        "            with open(registry_path, \"r\") as f:\n",
        "                registry = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            registry = {}\n",
        "    else:\n",
        "        registry = {}\n",
        "\n",
        "    # Actualizar el registro con el nuevo modelo\n",
        "    registry[model_save_name] = {\n",
        "        \"model_path\": model_path,\n",
        "        \"headers\": headers,\n",
        "        \"target_names\": target_names_serializable\n",
        "    }\n",
        "\n",
        "    # Escribir el registro de forma atómica usando atomic_write\n",
        "    try:\n",
        "        with atomic_write(registry_path, overwrite=True, encoding=\"utf-8\") as f:\n",
        "            json.dump(registry, f, indent=4)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al escribir el registro de forma atómica: {e}\")\n",
        "        raise\n",
        "\n",
        "    return model_path, registry\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# ENTRENAMIENTO INDIVIDUAL\n",
        "# ---------------------------\n",
        "def train_one_model_with_save(dataset_name, model_name, test_size, val1, val2, val3, val4):\n",
        "    \"\"\"Entrena un modelo, retorna la métrica, la figura, la tabla, el 'model_state' y los hiperparámetros.\"\"\"\n",
        "    df, target = load_dataset(dataset_name)\n",
        "    headers = df.columns.tolist()\n",
        "    # Extraer target_names si existen\n",
        "    dataset = available_datasets[dataset_name]()\n",
        "    target_names = dataset.get(\"target_names\", None)  # Podría ser None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Parametrización\n",
        "    if model_name in classification_models:\n",
        "        if model_name == \"Logistic Regression\":\n",
        "            hyperparams = {\"C\": val1, \"max_iter\": int(round(val2))}\n",
        "        elif model_name == \"KNN Classifier\":\n",
        "            hyperparams = {\"n_neighbors\": int(round(val1))}\n",
        "        elif model_name == \"Decision Tree Classifier\":\n",
        "            hyperparams = {\"max_depth\": int(round(val1)), \"min_samples_split\": int(round(val2))}\n",
        "        elif model_name == \"Random Forest Classifier\":\n",
        "            hyperparams = {\"n_estimators\": int(round(val1)), \"max_depth\": int(round(val2))}\n",
        "        elif model_name == \"SVC\":\n",
        "            hyperparams = {\"C\": val1, \"gamma\": val2}\n",
        "        else:\n",
        "            hyperparams = {}\n",
        "    else:\n",
        "        # Regresión\n",
        "        if model_name == \"Random Forest Regressor\":\n",
        "            hyperparams = {\"n_estimators\": int(round(val1)), \"max_depth\": int(round(val2))}\n",
        "        else:\n",
        "            hyperparams = {}\n",
        "\n",
        "    ModelClass = available_models.get(model_name)\n",
        "    if not ModelClass:\n",
        "        return \"Modelo no válido\", None, pd.DataFrame(), None, None\n",
        "\n",
        "    model = ModelClass(**hyperparams)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Métrica y visual\n",
        "    if model_name in classification_models:\n",
        "        metric = accuracy_score(y_test, y_pred)\n",
        "        metric_name = \"Accuracy\"\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
        "        ax.set_title(\"Confusion Matrix\")\n",
        "        table_df = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred})\n",
        "    else:\n",
        "        metric = r2_score(y_test, y_pred)\n",
        "        metric_name = \"R2 Score\"\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(y_test, y_pred)\n",
        "        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\", lw=2)\n",
        "        ax.set_xlabel(\"Actual\")\n",
        "        ax.set_ylabel(\"Predicted\")\n",
        "        ax.set_title(\"Actual vs Predicted\")\n",
        "        table_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
        "\n",
        "    model_state = {\n",
        "        \"model\": model,\n",
        "        \"headers\": headers,\n",
        "        \"target_names\": target_names\n",
        "    }\n",
        "    return f\"{metric_name}: {metric:.4f}\", fig, table_df, model_state, hyperparams\n",
        "\n",
        "def load_model_registry():\n",
        "    registry_path = \"model_registry.json\"\n",
        "    if os.path.exists(registry_path):\n",
        "        try:\n",
        "            with open(registry_path, \"r\") as f:\n",
        "                registry = json.load(f)\n",
        "            print(\"Registro cargado:\", registry)  # Debug\n",
        "            return list(registry.keys())\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error en decodificación del JSON.\")\n",
        "            return []\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# ---------------------------\n",
        "# UTILS PARA LA PREDICCIÓN\n",
        "# ---------------------------\n",
        "MAX_FEATURES = 20\n",
        "\n",
        "def refresh_model_list():\n",
        "    return gr.update(choices=load_model_registry())\n",
        "\n",
        "def update_prediction_ui(dataset_name):\n",
        "    \"\"\"Muestra/oculta textboxes según el dataset seleccionado (para pred. individual).\"\"\"\n",
        "    df, _ = load_dataset(dataset_name)\n",
        "    feature_names = df.columns.tolist()\n",
        "    n = min(len(feature_names), MAX_FEATURES)\n",
        "    updates = []\n",
        "    for i in range(MAX_FEATURES):\n",
        "        if i < n:\n",
        "            updates.append(gr.update(label=feature_names[i], value=\"\", visible=True))\n",
        "        else:\n",
        "            updates.append(gr.update(label=f\"Feature_{i}\", value=\"\", visible=False))\n",
        "    return updates\n",
        "\n",
        "def update_textboxes_from_saved_model(selected_model_name):\n",
        "    \"\"\"Si el modelo guardado tiene cabeceras, se muestran esas. De lo contrario, se ocultan.\"\"\"\n",
        "    registry_path = \"model_registry.json\"\n",
        "    if not os.path.exists(registry_path):\n",
        "        return [gr.update(visible=False) for _ in range(MAX_FEATURES)]\n",
        "    with open(registry_path, \"r\") as f:\n",
        "        registry = json.load(f)\n",
        "    if selected_model_name not in registry:\n",
        "        return [gr.update(visible=False) for _ in range(MAX_FEATURES)]\n",
        "    entry = registry[selected_model_name]\n",
        "    if isinstance(entry, dict):\n",
        "        headers = entry.get(\"headers\", [])\n",
        "    else:\n",
        "        headers = []\n",
        "    updates = []\n",
        "    n = min(len(headers), MAX_FEATURES)\n",
        "    for i in range(MAX_FEATURES):\n",
        "        if i < n:\n",
        "            updates.append(gr.update(label=headers[i], value=\"\", visible=True))\n",
        "        else:\n",
        "            updates.append(gr.update(visible=False))\n",
        "    return updates\n",
        "\n",
        "def predict_model_combined(selected_model_name, excel_file, *features):\n",
        "    \"\"\"Predicción combinada: si se sube un Excel -> pred masiva; si no, pred individual.\"\"\"\n",
        "    registry_path = \"model_registry.json\"\n",
        "    if not os.path.exists(registry_path):\n",
        "        return None, \"No hay modelos guardados.\"\n",
        "\n",
        "    with open(registry_path, \"r\") as f:\n",
        "        registry = json.load(f)\n",
        "\n",
        "    if selected_model_name not in registry:\n",
        "        return None, \"Modelo no encontrado en el registro.\"\n",
        "\n",
        "    model_path = registry[selected_model_name][\"model_path\"]\n",
        "    try:\n",
        "        with open(model_path, \"rb\") as ff:\n",
        "            loaded_data = pickle.load(ff)\n",
        "            model = loaded_data[\"model\"]\n",
        "            saved_headers = loaded_data[\"headers\"]\n",
        "            saved_target_names = loaded_data.get(\"target_names\", None)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error al cargar el modelo: {e}\"\n",
        "\n",
        "    # Función para mapear un índice numérico a la clase real, si target_names existen\n",
        "    def map_prediction(pred):\n",
        "        if saved_target_names is not None:\n",
        "            try:\n",
        "                return saved_target_names[int(pred)]\n",
        "            except Exception:\n",
        "                return pred  # Por si hay error al indexar\n",
        "        else:\n",
        "            return pred\n",
        "\n",
        "    # Modo masivo (archivo Excel)\n",
        "    if excel_file is not None:\n",
        "        try:\n",
        "            df = pd.read_excel(excel_file.name)\n",
        "        except Exception as e:\n",
        "            return None, f\"Error al leer el Excel: {e}\"\n",
        "\n",
        "        expected = set(saved_headers)\n",
        "        found = set(df.columns)\n",
        "        if not expected.issubset(found):\n",
        "            return None, f\"El Excel debe contener al menos estas columnas: {list(expected)}\"\n",
        "\n",
        "        X_df = df[saved_headers]\n",
        "        try:\n",
        "            preds = model.predict(X_df)\n",
        "        except Exception as e:\n",
        "            return None, f\"Error en la predicción: {e}\"\n",
        "\n",
        "        # Si es clasificación y hay target_names, convertimos el número a la clase\n",
        "        df['Predicción'] = [map_prediction(x) for x in preds]\n",
        "        return df, \"\"  # Retorna la tabla y deja en blanco el texto de pred individual\n",
        "\n",
        "    else:\n",
        "        # Modo individual (textboxes)\n",
        "        row_values = []\n",
        "        for i in range(len(saved_headers)):\n",
        "            val_str = features[i] if i < len(features) else \"\"\n",
        "            if val_str.strip() == \"\":\n",
        "                val = 0.0\n",
        "            else:\n",
        "                try:\n",
        "                    val = float(val_str)\n",
        "                except:\n",
        "                    return None, \"Error: todos los features deben ser numéricos.\"\n",
        "            row_values.append(val)\n",
        "\n",
        "        X_df = pd.DataFrame([row_values], columns=saved_headers)\n",
        "        try:\n",
        "            pred = model.predict(X_df)\n",
        "        except Exception as e:\n",
        "            return None, f\"Error en la predicción: {e}\"\n",
        "\n",
        "        predicted_class = map_prediction(pred[0])\n",
        "        return None, f\"Predicción: {predicted_class}\"\n",
        "\n",
        "# ---------------------------\n",
        "# INTERFAZ GRADIO PRINCIPAL\n",
        "# ---------------------------\n",
        "with gr.Blocks() as demo:\n",
        "    # ----- Pestaña 1: Dataset + Modelos ML -----\n",
        "    with gr.Tab(\"Dataset + Modelos ML\"):\n",
        "        gr.Markdown(\"## Visualización de Dataset y Comparación de Modelos (múltiples)\")\n",
        "\n",
        "        with gr.Row():\n",
        "            dataset_selector = gr.Dropdown(\n",
        "                choices=list(available_datasets.keys()),\n",
        "                value=\"Iris (Clasificación)\",\n",
        "                label=\"Selecciona un dataset\"\n",
        "            )\n",
        "            model_checkboxes = gr.CheckboxGroup(\n",
        "                choices=list(classification_models.keys()),\n",
        "                value=[\"Logistic Regression\", \"Random Forest Classifier\", \"SVC\", \"KNN Classifier\", \"Decision Tree Classifier\", \"Naive Bayes\"],\n",
        "                label=\"Modelos a entrenar\"\n",
        "            )\n",
        "            test_size_slider = gr.Slider(\n",
        "                minimum=0.1, maximum=0.5, value=0.3, step=0.05,\n",
        "                label=\"Proporción de datos de prueba\"\n",
        "            )\n",
        "        train_btn = gr.Button(\"Entrenar y Comparar\")\n",
        "\n",
        "        results_table = gr.Dataframe(\n",
        "            headers=[\"Modelo\", \"Precisión\"],\n",
        "            label=\"Resultados Ordenados por Precisión\"\n",
        "        )\n",
        "        accuracy_plot = gr.Plot(label=\"Gráfica de Precisión\")\n",
        "        csv_output = gr.File(label=\"Reporte CSV\")\n",
        "\n",
        "        train_btn.click(\n",
        "            fn=full_training,\n",
        "            inputs=[dataset_selector, model_checkboxes, test_size_slider],\n",
        "            outputs=[results_table, accuracy_plot, csv_output]\n",
        "        )\n",
        "\n",
        "        # Vista del dataset\n",
        "        initial_df, initial_label = show_dataset_with_target(\"Iris (Clasificación)\")\n",
        "        dataset_label = gr.Markdown(value=initial_label)\n",
        "        data_table = gr.Dataframe(value=initial_df, label=None)\n",
        "\n",
        "        dataset_selector.change(\n",
        "            fn=show_dataset_with_target,\n",
        "            inputs=dataset_selector,\n",
        "            outputs=[data_table, dataset_label]\n",
        "        )\n",
        "        dataset_selector.change(\n",
        "            fn=update_model_choices,\n",
        "            inputs=dataset_selector,\n",
        "            outputs=model_checkboxes\n",
        "        )\n",
        "\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"EDA\"):\n",
        "                eda_table_nested = gr.Dataframe(\n",
        "                    value=run_eda(\"Iris (Clasificación)\"),\n",
        "                    label=\"Descripción Estadística\"\n",
        "                )\n",
        "                eda_plot_nested = gr.Plot(\n",
        "                    value=plot_eda(\"Iris (Clasificación)\"),\n",
        "                    label=\"Distribución de Features\"\n",
        "                )\n",
        "            with gr.Tab(\"PCA\"):\n",
        "                pca_output_nested = gr.Plot(\n",
        "                    value=run_pca(\"Iris (Clasificación)\"),\n",
        "                    label=\"Visualización PCA\"\n",
        "                )\n",
        "\n",
        "        dataset_selector.change(fn=run_eda, inputs=dataset_selector, outputs=eda_table_nested)\n",
        "        dataset_selector.change(fn=plot_eda, inputs=dataset_selector, outputs=eda_plot_nested)\n",
        "        dataset_selector.change(fn=run_pca, inputs=dataset_selector, outputs=pca_output_nested)\n",
        "\n",
        "    # ----- Pestaña 2: Entrenamiento (1 modelo + guardado) -----\n",
        "    with gr.Tab(\"Entrenamiento\"):\n",
        "        gr.Markdown(\"## Entrenamiento (un solo modelo) con hiperparámetros\")\n",
        "\n",
        "        dataset_selector_train = gr.Dropdown(\n",
        "            choices=list(available_datasets.keys()),\n",
        "            value=\"Iris (Clasificación)\",\n",
        "            label=\"Selecciona un dataset\"\n",
        "        )\n",
        "        model_selector_train = gr.Dropdown(\n",
        "            choices=list(classification_models.keys()),\n",
        "            value=list(classification_models.keys())[0],\n",
        "            label=\"Selecciona un modelo\"\n",
        "        )\n",
        "\n",
        "        dataset_selector_train.change(\n",
        "            fn=update_model_choices,\n",
        "            inputs=dataset_selector_train,\n",
        "            outputs=model_selector_train\n",
        "        )\n",
        "\n",
        "        test_size_slider_train = gr.Slider(\n",
        "            minimum=0.1, maximum=0.5, value=0.3, step=0.05,\n",
        "            label=\"Proporción de datos de prueba\"\n",
        "        )\n",
        "\n",
        "        # Sliders: LR por defecto\n",
        "        lr_C_params = fix_slider_params(model_params_demo[\"Logistic Regression\"][\"C\"])\n",
        "        lr_C_params.pop(\"type\", None)\n",
        "        lr_max_iter_params = fix_slider_params(model_params_demo[\"Logistic Regression\"][\"max_iter\"])\n",
        "        lr_max_iter_params.pop(\"type\", None)\n",
        "\n",
        "        rf_n_estimators_params = fix_slider_params(model_params_demo[\"Random Forest Regressor\"][\"n_estimators\"])\n",
        "        rf_n_estimators_params.pop(\"type\", None)\n",
        "        rf_max_depth_params = fix_slider_params(model_params_demo[\"Random Forest Regressor\"][\"max_depth\"])\n",
        "        rf_max_depth_params.pop(\"type\", None)\n",
        "\n",
        "        c_slider = gr.Slider(\n",
        "            label=lr_C_params.get(\"label\", \"C\"),\n",
        "            minimum=lr_C_params[\"minimum\"],\n",
        "            maximum=lr_C_params[\"maximum\"],\n",
        "            value=lr_C_params[\"value\"],\n",
        "            step=lr_C_params[\"step\"],\n",
        "            visible=True\n",
        "        )\n",
        "        max_iter_slider = gr.Slider(\n",
        "            label=lr_max_iter_params.get(\"label\", \"Iteraciones Máx.\"),\n",
        "            minimum=lr_max_iter_params[\"minimum\"],\n",
        "            maximum=lr_max_iter_params[\"maximum\"],\n",
        "            value=lr_max_iter_params[\"value\"],\n",
        "            step=lr_max_iter_params[\"step\"],\n",
        "            visible=True\n",
        "        )\n",
        "        n_estimators_slider = gr.Slider(\n",
        "            label=rf_n_estimators_params.get(\"label\", \"n_estimators\"),\n",
        "            minimum=rf_n_estimators_params[\"minimum\"],\n",
        "            maximum=rf_n_estimators_params[\"maximum\"],\n",
        "            value=rf_n_estimators_params[\"value\"],\n",
        "            step=rf_n_estimators_params[\"step\"],\n",
        "            visible=False\n",
        "        )\n",
        "        max_depth_slider = gr.Slider(\n",
        "            label=rf_max_depth_params.get(\"label\", \"max_depth\"),\n",
        "            minimum=rf_max_depth_params[\"minimum\"],\n",
        "            maximum=rf_max_depth_params[\"maximum\"],\n",
        "            value=rf_max_depth_params[\"value\"],\n",
        "            step=rf_max_depth_params[\"step\"],\n",
        "            visible=False\n",
        "        )\n",
        "\n",
        "        def show_model_params(model_name):\n",
        "            \"\"\"Muestra/oculta sliders según el modelo elegido.\"\"\"\n",
        "            if model_name == \"Logistic Regression\":\n",
        "                return (\n",
        "                    gr.update(visible=True, value=lr_C_params[\"value\"]),\n",
        "                    gr.update(visible=True, value=lr_max_iter_params[\"value\"]),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False)\n",
        "                )\n",
        "            elif model_name == \"KNN Classifier\":\n",
        "                return (\n",
        "                    gr.update(visible=True, value=model_params_demo[\"KNN Classifier\"][\"n_neighbors\"][\"value\"]),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False)\n",
        "                )\n",
        "            elif model_name == \"Decision Tree Classifier\":\n",
        "                return (\n",
        "                    gr.update(visible=True, value=model_params_demo[\"Decision Tree Classifier\"][\"max_depth\"][\"value\"]),\n",
        "                    gr.update(visible=True, value=model_params_demo[\"Decision Tree Classifier\"][\"min_samples_split\"][\"value\"]),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False)\n",
        "                )\n",
        "            elif model_name == \"Random Forest Classifier\":\n",
        "                return (\n",
        "                    gr.update(visible=True, value=model_params_demo[\"Random Forest Classifier\"][\"n_estimators\"][\"value\"]),\n",
        "                    gr.update(visible=True, value=model_params_demo[\"Random Forest Classifier\"][\"max_depth\"][\"value\"]),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False)\n",
        "                )\n",
        "            elif model_name == \"SVC\":\n",
        "                return (\n",
        "                    gr.update(visible=True, value=model_params_demo[\"SVC\"][\"C\"][\"value\"]),\n",
        "                    gr.update(visible=True, value=model_params_demo[\"SVC\"][\"gamma\"][\"value\"]),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False)\n",
        "                )\n",
        "            elif model_name == \"Random Forest Regressor\":\n",
        "                return (\n",
        "                    gr.update(visible=True, value=model_params_demo[\"Random Forest Regressor\"][\"n_estimators\"][\"value\"]),\n",
        "                    gr.update(visible=True, value=model_params_demo[\"Random Forest Regressor\"][\"max_depth\"][\"value\"]),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(visible=False)\n",
        "                )\n",
        "            else:\n",
        "                return (\n",
        "                    gr.update(visible=False), gr.update(visible=False),\n",
        "                    gr.update(visible=False), gr.update(visible=False)\n",
        "                )\n",
        "\n",
        "        model_selector_train.change(\n",
        "            fn=show_model_params,\n",
        "            inputs=model_selector_train,\n",
        "            outputs=[c_slider, max_iter_slider, n_estimators_slider, max_depth_slider]\n",
        "        )\n",
        "\n",
        "        train_button_single = gr.Button(\"Entrenar (1 modelo)\")\n",
        "        metric_output = gr.Markdown(\"Sin entrenamiento todavía\")\n",
        "        pred_plot = gr.Plot(label=\"Gráfica de Resultados\")\n",
        "        pred_table = gr.Dataframe(label=\"Tabla de Predicciones\")\n",
        "        trained_model_state = gr.State()\n",
        "\n",
        "        def train_model(dataset_name, model_name, test_size, val1, val2, val3, val4):\n",
        "            msg, fig, table, model_state, hyperparams = train_one_model_with_save(\n",
        "                dataset_name, model_name, test_size, val1, val2, val3, val4\n",
        "            )\n",
        "            return msg, fig, table, model_state\n",
        "\n",
        "        train_button_single.click(\n",
        "            fn=train_model,\n",
        "            inputs=[\n",
        "                dataset_selector_train, model_selector_train, test_size_slider_train,\n",
        "                c_slider, max_iter_slider, n_estimators_slider, max_depth_slider\n",
        "            ],\n",
        "            outputs=[metric_output, pred_plot, pred_table, trained_model_state]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            model_save_name_input = gr.Textbox(\n",
        "                label=\"Nombre para guardar el modelo\",\n",
        "                placeholder=\"ej: mi_modelo.pkl\"\n",
        "            )\n",
        "            save_button = gr.Button(\"Guardar modelo\")\n",
        "\n",
        "        def save_trained_model(model_state, save_name):\n",
        "            if model_state is None:\n",
        "                return \"No hay modelo entrenado para guardar.\"\n",
        "            model_obj = model_state.get(\"model\")\n",
        "            headers = model_state.get(\"headers\")\n",
        "            target_names = model_state.get(\"target_names\")\n",
        "            model_path, registry = save_model_locally(model_obj, save_name, headers, target_names)\n",
        "            return f\"Modelo guardado en: {model_path}\\nRegistro:\\n{json.dumps(registry, indent=2)}\"\n",
        "\n",
        "        save_button.click(\n",
        "            fn=save_trained_model,\n",
        "            inputs=[trained_model_state, model_save_name_input],\n",
        "            outputs=metric_output\n",
        "        )\n",
        "\n",
        "    # ----- Pestaña 3: \"Predicción\" -----\n",
        "    with gr.Tab(\"Predicción\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Predicción\n",
        "\n",
        "        1. **Selecciona un modelo guardado** (abajo).\n",
        "        2. *Opcionalmente*, selecciona un dataset para ajustar los campos de la predicción individual.\n",
        "        3. *Opcionalmente*, sube un archivo Excel con las columnas del modelo para predicción masiva.\n",
        "\n",
        "        - Si subes Excel, verás la predicción en formato de tabla.\n",
        "        - Si *no* subes Excel, puedes usar los inputs individuales para hacer una sola predicción.\n",
        "        \"\"\")\n",
        "\n",
        "        # --- Reorganizado: primero dataset, luego modelo ---\n",
        "        dataset_selector_pred = gr.Dropdown(\n",
        "            choices=list(available_datasets.keys()),\n",
        "            value=\"Iris (Clasificación)\",\n",
        "            label=\"Selecciona un dataset (para actualizar inputs individuales)\"\n",
        "        )\n",
        "\n",
        "        model_registry_dropdown = gr.Dropdown(\n",
        "            choices=load_model_registry(),\n",
        "            label=\"Modelos guardados\"\n",
        "        )\n",
        "        refresh_models_btn = gr.Button(\"Refrescar modelos\")\n",
        "\n",
        "        refresh_models_btn.click(fn=refresh_model_list, inputs=[], outputs=model_registry_dropdown)\n",
        "\n",
        "        # Inputs para predicción individual (hasta MAX_FEATURES)\n",
        "        feature_textboxes = []\n",
        "        for i in range(MAX_FEATURES):\n",
        "            txt = gr.Textbox(label=f\"Feature_{i}\", visible=False)\n",
        "            feature_textboxes.append(txt)\n",
        "\n",
        "        dataset_selector_pred.change(\n",
        "            fn=update_prediction_ui,\n",
        "            inputs=dataset_selector_pred,\n",
        "            outputs=feature_textboxes\n",
        "        )\n",
        "\n",
        "\n",
        "        # Al cambiar de modelo, actualizamos inputs con las cabeceras guardadas\n",
        "        model_registry_dropdown.change(\n",
        "            fn=update_textboxes_from_saved_model,\n",
        "            inputs=model_registry_dropdown,\n",
        "            outputs=feature_textboxes\n",
        "        )\n",
        "\n",
        "        # Botón para generar plantilla Excel\n",
        "        plantilla_btn = gr.Button(\"Generar plantilla Excel\")\n",
        "        plantilla_output = gr.File(label=\"Descargar plantilla Excel\", interactive=False)\n",
        "\n",
        "        def generate_excel_template(selected_model_name):\n",
        "            registry_path = \"model_registry.json\"\n",
        "            if not os.path.exists(registry_path):\n",
        "                return None\n",
        "            with open(registry_path, \"r\") as f:\n",
        "                registry = json.load(f)\n",
        "            if selected_model_name not in registry:\n",
        "                return None\n",
        "            entry = registry[selected_model_name]\n",
        "            if isinstance(entry, dict):\n",
        "                headers = entry.get(\"headers\", [])\n",
        "            else:\n",
        "                headers = []\n",
        "            df = pd.DataFrame(columns=headers)\n",
        "            temp_path = \"plantilla.xlsx\"\n",
        "            df.to_excel(temp_path, index=False)\n",
        "            return temp_path\n",
        "\n",
        "        plantilla_btn.click(\n",
        "            fn=generate_excel_template,\n",
        "            inputs=model_registry_dropdown,\n",
        "            outputs=plantilla_output\n",
        "        )\n",
        "\n",
        "        # Componente de carga de Excel\n",
        "        excel_upload = gr.File(label=\"Cargar archivo Excel (opcional)\", file_types=[\".xlsx\", \".xls\"])\n",
        "\n",
        "        predict_btn = gr.Button(\"Predecir\")\n",
        "        bulk_output = gr.Dataframe(label=\"Predicción en modo masivo (Excel)\")\n",
        "        single_output = gr.Markdown(\"Resultado de predicción individual\")\n",
        "\n",
        "        predict_btn.click(\n",
        "            fn=predict_model_combined,\n",
        "            inputs=[model_registry_dropdown, excel_upload] + feature_textboxes,\n",
        "            outputs=[bulk_output, single_output]\n",
        "        )\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "38tm4diBGhsm"
      }
    }
  ]
}